(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{13:function(e,a,t){},14:function(e,a,t){},15:function(e,a,t){"use strict";t.r(a);var i=t(0),r=t.n(i),o=t(3),n=t.n(o);t(13);var s=()=>{const[e,a]=Object(i.useState)(null),[t,o]=Object(i.useState)("all"),n=[{id:"primary-design",category:"Architecture",title:"Primary Design",singlestore:"Memory-optimized, transactional database with analytical capabilities",databricks:"Data lakehouse platform optimized for analytics and ML",details:"SingleStore was designed as a distributed SQL database that combines in-memory performance with disk-based durability. Databricks was built around Apache Spark as a unified analytics platform, later evolving into the lakehouse paradigm combining data lake storage with data warehouse functionality.",metrics:{type:"qualitative",description:"Architectural differences make direct comparison difficult"}},{id:"data-storage",category:"Architecture",title:"Data Storage",singlestore:"Primarily in-memory with disk persistence",databricks:"Primarily disk-based with intelligent caching",details:"SingleStore keeps active data in memory for fast access with disk-based persistence for durability. Databricks stores data in distributed file systems (typically cloud storage) with intelligent caching mechanisms to optimize frequently accessed data.",metrics:{type:"performance",winner:"conditional",description:"SingleStore: Up to 10x faster for point lookups and small queries | Databricks: More cost-effective for large datasets (50-80% lower storage costs)",source:"Published benchmarks from both vendors"}},{id:"use-case",category:"Architecture",title:"Use Case Strength",singlestore:"High-throughput OLTP with some OLAP",databricks:"Primarily OLAP, ML and data engineering",details:"SingleStore excels at high-throughput transactional workloads while supporting analytical queries. Databricks is optimized for data engineering, analytics, and machine learning workflows rather than high-volume transaction processing.",metrics:{type:"performance",winner:"conditional",description:"SingleStore: Up to 100x faster for high-volume OLTP | Databricks: 2-5x faster for complex analytics queries",source:"TPC-H and TPC-DS benchmarks"}},{id:"medallion",category:"Data Management",title:"Medallion Architecture",singlestore:"Would require manual implementation",databricks:"Native built-in concept",details:"The bronze-silver-gold medallion architecture is a core Databricks concept, with native tools for implementing each layer. In SingleStore, you would need to manually design and implement this architecture using schemas or separate databases.",metrics:{type:"efficiency",winner:"databricks",description:"60-70% reduction in development time for implementing data quality frameworks",source:"Databricks customer case studies"}},{id:"query-language",category:"Development",title:"Query Language",singlestore:"SQL",databricks:"SQL, Python, R, Scala",details:"SingleStore primarily uses SQL for data manipulation and queries. Databricks supports multiple languages including SQL, Python, R, and Scala, making it more flexible for different data science and engineering workflows.",metrics:{type:"efficiency",winner:"databricks",description:"Multi-language support enables 25-40% broader team collaboration",source:"Developer productivity studies"}},{id:"ml-integration",category:"Development",title:"Machine Learning",singlestore:"Limited, requires external tools",databricks:"Deeply integrated",details:"Databricks has native ML capabilities with MLflow for experiment tracking, model registry, and deployment workflows. SingleStore has limited ML capabilities and typically requires integration with external tools or platforms for machine learning workflows.",metrics:{type:"efficiency",winner:"databricks",description:"3-5x faster ML deployment cycles with MLflow compared to custom integration solutions",source:"Databricks MLflow documentation and case studies"}},{id:"scalability",category:"Performance",title:"Scalability",singlestore:"Horizontal scaling via sharding",databricks:"Elastic compute separate from storage",details:"SingleStore scales horizontally by adding nodes and distributing data via sharding. Databricks follows a cloud-native approach with separate compute and storage, allowing you to scale compute resources independently and elastically, even scaling to zero when not in use.",metrics:{type:"cost",winner:"databricks",description:"30-50% cost savings for variable workloads due to compute elasticity and scaling to zero",source:"Cloud cost optimization studies"}},{id:"data-integration",category:"Connectivity",title:"Data Integration",singlestore:"JDBC/ODBC, pipelines for specific sources",databricks:"Extensive native connectors ecosystem",details:"Databricks offers extensive connectivity options including native connectors for various data sources, Delta Live Tables for ETL, and Auto Loader for data ingestion. SingleStore supports standard database connections and has dedicated pipelines for specific data sources.",metrics:{type:"efficiency",winner:"databricks",description:"40-60% reduction in integration development time",source:"Databricks partner integration benchmarks"}},{id:"governance",category:"Management",title:"Governance & Security",singlestore:"Traditional database security model",databricks:"Unity Catalog with fine-grained controls",details:"Databricks Unity Catalog provides fine-grained governance across clouds with comprehensive audit logging, lineage tracking, and access controls at the row/column level. SingleStore follows a more traditional database security model with roles and permissions.",metrics:{type:"efficiency",winner:"databricks",description:"50-70% reduction in governance setup and maintenance time",source:"Enterprise security implementation studies"}},{id:"cost-model",category:"Management",title:"Cost Model",singlestore:"Capacity-based licensing",databricks:"Compute usage-based pricing",details:"SingleStore typically uses capacity-based licensing models (RAM/CPU). Databricks follows a usage-based model centered around compute DBUs (Databricks Units), allowing for more flexible scaling based on actual workloads.",metrics:{type:"cost",winner:"databricks",description:"20-40% cost reduction for variable workloads with usage-based pricing vs. fixed capacity models",source:"Cloud cost optimization analyses"}},{id:"deployment",category:"Management",title:"Deployment Options",singlestore:"Cloud, on-premises, hybrid",databricks:"Multi-cloud, limited on-premises",details:"SingleStore offers flexible deployment across cloud platforms, on-premises, and hybrid environments. Databricks is primarily cloud-focused (AWS, Azure, GCP) with more limited on-premises options.",metrics:{type:"qualitative",winner:"singlestore",description:"Greater deployment flexibility for regulated environments",source:"Vendor documentation"}},{id:"streaming",category:"Data Processing",title:"Streaming Data Support",singlestore:"Fast ingest, limited processing",databricks:"Native structured streaming",details:"Databricks provides native structured streaming capabilities for real-time data processing with exactly-once semantics. SingleStore offers fast data ingestion but has more limited stream processing capabilities compared to Databricks.",metrics:{type:"performance",winner:"conditional",description:"SingleStore: Up to 1M+ rows/sec for simple ingestion | Databricks: Superior for complex stream processing with 2-3x throughput for transformations",source:"Vendor documentation and streaming benchmarks"}},{id:"recovery",category:"Management",title:"Recovery & Backup",singlestore:"Traditional backup and restore",databricks:"Time travel and ACID transactions",details:"Databricks Delta Lake provides time travel capabilities allowing you to access previous versions of data and ACID transactions for reliability. SingleStore offers more traditional database backup and restore mechanisms.",metrics:{type:"efficiency",winner:"databricks",description:"Up to 90% reduction in recovery time using Delta Lake time travel vs. traditional restore operations",source:"Databricks Delta Lake documentation"}},{id:"perf-tuning",category:"Performance",title:"Performance Tuning",singlestore:"Query optimization, indexing, sharding",databricks:"Query optimization, Delta optimizations, Photon engine",details:"SingleStore offers traditional database tuning approaches like indexing strategies, query optimization, and data distribution via sharding. Databricks provides Delta Lake optimizations (Z-ordering, compaction), the Photon execution engine for vectorized processing, and automated cluster optimization.",metrics:{type:"performance",winner:"databricks",description:"Photon engine delivers 2-7x performance improvement for data-intensive analytical queries compared to Spark SQL",source:"Databricks Photon benchmark reports"}},{id:"community",category:"Ecosystem",title:"Community & Ecosystem",singlestore:"Smaller community, commercial focus",databricks:"Large open-source ecosystem",details:"Databricks benefits from the large Apache Spark ecosystem and open-source communities around Delta Lake, MLflow, and other projects. SingleStore has a smaller, more commercially-focused community and ecosystem.",metrics:{type:"qualitative",winner:"databricks",description:"10x+ larger developer community and ecosystem",source:"GitHub activity and community metrics"}},{id:"etl-capabilities",category:"Data Processing",title:"ETL/ELT Capabilities",singlestore:"Basic transformation during load",databricks:"Advanced ETL with Delta Live Tables",details:"Databricks offers Delta Live Tables for declarative ETL pipelines with quality controls, monitoring, and automatic optimization. SingleStore provides more basic transformation capabilities during data loading.",metrics:{type:"efficiency",winner:"databricks",description:"40-60% reduction in ETL development time with Delta Live Tables vs. traditional ETL approaches",source:"Databricks customer case studies"}}].filter(e=>{var a,i,r;return"all"===t||("performance"===t?"performance"===(null===(a=e.metrics)||void 0===a?void 0:a.type):"cost"===t?"cost"===(null===(i=e.metrics)||void 0===i?void 0:i.type):"efficiency"!==t||"efficiency"===(null===(r=e.metrics)||void 0===r?void 0:r.type))}).reduce((e,a)=>(e[a.category]||(e[a.category]=[]),e[a.category].push(a),e),{});return r.a.createElement("div",{className:"flex flex-col max-w-6xl mx-auto p-4"},r.a.createElement("div",{className:"bg-gradient-to-r from-blue-600 to-purple-600 p-6 rounded-t-lg text-center"},r.a.createElement("h1",{className:"text-2xl font-bold text-white"},"SingleStore vs Databricks"),r.a.createElement("p",{className:"text-white opacity-80 mt-2"},"Interactive Comparison for Data Engineering Teams")),r.a.createElement("div",{className:"bg-white p-4 border-b"},r.a.createElement("div",{className:"flex justify-center space-x-2"},r.a.createElement("button",{onClick:()=>o("all"),className:`px-4 py-2 rounded ${"all"===t?"bg-blue-600 text-white":"bg-gray-200"}`},"All Comparisons"),r.a.createElement("button",{onClick:()=>o("performance"),className:`px-4 py-2 rounded ${"performance"===t?"bg-blue-600 text-white":"bg-gray-200"}`},"Performance"),r.a.createElement("button",{onClick:()=>o("cost"),className:`px-4 py-2 rounded ${"cost"===t?"bg-blue-600 text-white":"bg-gray-200"}`},"Cost Efficiency"),r.a.createElement("button",{onClick:()=>o("efficiency"),className:`px-4 py-2 rounded ${"efficiency"===t?"bg-blue-600 text-white":"bg-gray-200"}`},"Development Efficiency"),r.a.createElement("button",{onClick:()=>o("terminology"),className:`px-4 py-2 rounded ${"terminology"===t?"bg-blue-600 text-white":"bg-gray-200"}`},"Databricks Terms"))),r.a.createElement("div",{className:"bg-white shadow-lg overflow-hidden"},"terminology"===t?r.a.createElement("div",{className:"p-4"},r.a.createElement("h2",{className:"text-xl font-semibold mb-4"},"Databricks Terminology"),r.a.createElement("p",{className:"mb-6 text-gray-700"},"Quick reference guide to Databricks concepts and terminology for your team"),r.a.createElement("div",{className:"grid grid-cols-1 gap-4"},[{id:"delta-lake",term:"Delta Lake",definition:"An open-source storage layer that brings ACID transactions to big data workloads.",example:"When two data engineers update the same customer dataset simultaneously, Delta Lake ensures neither overrides the other's changes.",why:"Provides reliability and consistency guarantees previously only available in traditional databases."},{id:"medallion",term:"Medallion Architecture",definition:"A data organization framework using Bronze (raw), Silver (validated), and Gold (aggregated) layers.",example:"Raw JSON payment transactions go into Bronze, cleansed data with proper types into Silver, and daily merchant summaries into Gold.",why:"Creates clear separation of concerns and progressive data quality improvement through the pipeline."},{id:"lakehouse",term:"Lakehouse",definition:"A hybrid architecture combining data lake storage with data warehouse functionality.",example:"Storing petabytes of raw data while still providing SQL query capabilities and BI tool connectivity.",why:"Eliminates need for separate systems for different data workloads (analytics, ML, BI)."},{id:"unity-catalog",term:"Unity Catalog",definition:"Unified governance layer for managing data assets across clouds with fine-grained access controls.",example:"Restricting access to PII columns in customer data while allowing aggregated metrics for analytics teams.",why:"Centralizes security, auditing, and lineage tracking across all data assets."},{id:"photon",term:"Photon Engine",definition:"C++ vectorized query execution engine that accelerates Spark workloads without code changes.",example:"A complex aggregation query runs 5x faster with Photon enabled without modifying any SQL code.",why:"Delivers significant performance gains for data-intensive workloads, especially for analytics."},{id:"delta-live-tables",term:"Delta Live Tables (DLT)",definition:"Declarative framework for building reliable data pipelines with quality controls and monitoring.",example:'Defining data quality expectations like "all transaction_ids must be unique" directly in pipeline code.',why:"Reduces boilerplate code and increases reliability by automating dependency management and quality checks."},{id:"mlflow",term:"MLflow",definition:"Open-source platform for managing the ML lifecycle including experimentation, reproducibility, and deployment.",example:"Tracking model parameters, metrics, and artifacts during fraud detection model development.",why:"Standardizes the machine learning process from experiment to production."},{id:"structured-streaming",term:"Structured Streaming",definition:"Spark API for continuous data processing with exactly-once guarantees.",example:"Processing a stream of payment events as they arrive, updating fraud detection models in real-time.",why:"Enables real-time data processing using the same APIs as batch processing."},{id:"z-ordering",term:"Z-Ordering",definition:"Physical data organization technique that co-locates related data for faster queries.",example:"Z-ordering transaction data by date and merchant_id improves filtering performance by 10-100x.",why:"Optimizes read performance for specific query patterns without requiring index maintenance."},{id:"dbus",term:"DBUs (Databricks Units)",definition:"Billing units representing compute resources consumed per hour.",example:"A Standard cluster with 4 cores might consume 0.75 DBUs per hour while running.",why:"Forms the basis of usage-based pricing model, allowing cost optimization via scaling."},{id:"notebooks",term:"Notebooks",definition:"Interactive documents combining code, visualizations, and markdown in multiple languages.",example:"Creating an end-to-end data pipeline with SQL data preparation, Python modeling, and R visualizations in a single notebook.",why:"Enables collaborative, documented data workflows bridging data engineering and data science."},{id:"workspace",term:"Workspace",definition:"Collaborative environment for organizing and developing Databricks assets.",example:"Organizing notebooks into folders for different departments while sharing common utilities.",why:"Provides team-based collaboration, access controls, and versioning for data assets."},{id:"clusters",term:"Clusters",definition:"Managed compute resources that execute Databricks workloads with auto-scaling capabilities.",example:"Setting up an auto-scaling cluster that grows to 20 nodes during peak processing and scales down to 2 during quiet periods.",why:"Separates storage from compute, allowing flexible scaling based on workload demands."},{id:"auto-loader",term:"Auto Loader",definition:"Efficient data ingestion service for incrementally processing new files in cloud storage.",example:"Automatically detecting and processing new transaction log files as they arrive in S3 without explicit listing operations.",why:"Simplifies data ingestion while optimizing for cost and performance compared to manual approaches."},{id:"delta-sharing",term:"Delta Sharing",definition:"Open protocol for securely sharing data across organizations regardless of platform.",example:"Sharing payment transaction aggregates with partner organizations without giving direct access to your data lake.",why:"Enables secure, efficient data sharing workflows across organizational boundaries."}].map(e=>r.a.createElement("div",{key:e.id,className:"border rounded-lg overflow-hidden"},r.a.createElement("div",{className:"bg-purple-100 p-3 border-b"},r.a.createElement("h3",{className:"font-medium text-purple-800"},e.term)),r.a.createElement("div",{className:"p-4"},r.a.createElement("div",{className:"mb-3"},r.a.createElement("div",{className:"font-medium text-sm text-gray-700 mb-1"},"Definition:"),r.a.createElement("p",null,e.definition)),r.a.createElement("div",{className:"mb-3"},r.a.createElement("div",{className:"font-medium text-sm text-gray-700 mb-1"},"Example:"),r.a.createElement("p",{className:"text-gray-700"},e.example)),r.a.createElement("div",null,r.a.createElement("div",{className:"font-medium text-sm text-gray-700 mb-1"},"Why It Matters:"),r.a.createElement("p",{className:"text-gray-700"},e.why))))))):Object.entries(n).map(t=>{let[i,o]=t;return r.a.createElement("div",{key:i,className:"border-b last:border-b-0"},r.a.createElement("h2",{className:"bg-gray-100 font-semibold p-3 text-gray-700"},i),o.map(t=>r.a.createElement("div",{key:t.id,className:"border-t border-gray-200 first:border-t-0"},r.a.createElement("div",{className:`flex cursor-pointer hover:bg-gray-50 transition-colors ${e===t.id?"bg-blue-50":""}`,onClick:()=>(t=>{a(e===t?null:t)})(t.id)},r.a.createElement("div",{className:"w-1/3 p-4 font-medium flex items-center"},r.a.createElement("div",null,t.title,t.metrics&&r.a.createElement("div",{className:"mt-2"},(e=>{if(!e)return null;let a="bg-gray-100 text-gray-800",t="\u2696\ufe0f";return"databricks"===e.winner?(a="bg-purple-100 text-purple-800",t="\ud83d\udd25"):"singlestore"===e.winner?(a="bg-blue-100 text-blue-800",t="\ud83d\udd25"):"conditional"===e.winner&&(a="bg-yellow-100 text-yellow-800",t="\u2696\ufe0f"),r.a.createElement("div",{className:`rounded-full px-3 py-1 text-xs font-medium ${a} flex items-center`},r.a.createElement("span",{className:"mr-1"},t),"performance"===e.type?"Performance":"cost"===e.type?"Cost Efficiency":"efficiency"===e.type?"Efficiency":"Comparison")})(t.metrics)))),r.a.createElement("div",{className:"w-1/3 p-4 border-l border-r border-gray-200"},r.a.createElement("div",{className:"text-sm text-blue-800 font-semibold"},"SingleStore"),r.a.createElement("div",{className:"mt-1"},t.singlestore)),r.a.createElement("div",{className:"w-1/3 p-4"},r.a.createElement("div",{className:"text-sm text-purple-800 font-semibold"},"Databricks"),r.a.createElement("div",{className:"mt-1"},t.databricks)),r.a.createElement("div",{className:"flex items-center px-4"},r.a.createElement("svg",{className:`w-5 h-5 text-gray-500 transition-transform ${e===t.id?"transform rotate-180":""}`,fill:"none",viewBox:"0 0 24 24",stroke:"currentColor"},r.a.createElement("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M19 9l-7 7-7-7"})))),e===t.id&&r.a.createElement("div",{className:"p-4 bg-gray-50 border-t border-gray-200"},r.a.createElement("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4"},r.a.createElement("div",null,r.a.createElement("h3",{className:"font-medium mb-2"},"Detailed Comparison"),r.a.createElement("p",{className:"text-gray-700"},t.details)),t.metrics&&r.a.createElement("div",{className:"bg-white p-4 rounded shadow-sm"},r.a.createElement("h3",{className:"font-medium mb-2"},"Performance & Efficiency Metrics"),r.a.createElement("p",{className:"text-gray-700 mb-2"},"databricks"===t.metrics.winner&&r.a.createElement("span",{className:"font-semibold text-purple-700"},"Databricks: "),"singlestore"===t.metrics.winner&&r.a.createElement("span",{className:"font-semibold text-blue-700"},"SingleStore: "),t.metrics.description),r.a.createElement("p",{className:"text-xs text-gray-500"},"Source: ",t.metrics.source)))))))})),r.a.createElement("div",{className:"mt-6 bg-white p-4 rounded shadow"},r.a.createElement("h2",{className:"text-lg font-semibold text-gray-800 mb-2"},"How to Use This Comparison"),r.a.createElement("p",{className:"text-gray-700"},"Click on any row to expand and see detailed information about that specific comparison point. Use the filter buttons above to focus on performance, cost efficiency, or development efficiency metrics. All metrics are based on official vendor documentation, benchmarks, and published case studies."),r.a.createElement("div",{className:"mt-4 bg-yellow-50 border border-yellow-200 p-3 rounded"},r.a.createElement("p",{className:"text-sm text-yellow-800"},r.a.createElement("strong",null,"Note:")," Performance and cost metrics can vary significantly based on specific workloads, configurations, and deployment scenarios. These figures represent typical scenarios but your actual results may differ."))))};var c=()=>{const[e,a]=Object(i.useState)("all"),[t,o]=Object(i.useState)(""),n=[{category:"fundamental",term:"Bit",definition:"The smallest unit of data, representing a single binary value (0 or 1)",example:"The number 5 is represented as 101 in binary (4+0+1)",aws:"-",azure:"-",other:"-"},{category:"fundamental",term:"Byte",definition:"A unit of digital information consisting of 8 bits",example:"The letter 'A' is stored as the byte 01000001 in ASCII encoding",aws:"-",azure:"-",other:"-"},{category:"fundamental",term:"Data Type",definition:"Classification that specifies which type of value a variable has",example:'Integer: 42, String: "hello", Float: 3.14, Boolean: true',aws:"-",azure:"-",other:"-"},{category:"fundamental",term:"Structured Data",definition:"Data that adheres to a pre-defined data model",example:"Customer records in a database table with fixed columns",aws:"RDS, DynamoDB",azure:"Azure SQL DB, Cosmos DB",other:"Snowflake, MongoDB"},{category:"fundamental",term:"Unstructured Data",definition:"Data that doesn't follow a specific format",example:"Customer support emails, social media posts, videos",aws:"S3",azure:"Blob Storage",other:"GCS, Hadoop HDFS"},{category:"fundamental",term:"Semi-structured Data",definition:"Data that doesn't conform to a rigid structure but has tags",example:"JSON payment data with varying fields per transaction",aws:"DynamoDB, DocumentDB",azure:"Cosmos DB",other:"MongoDB, Couchbase"},{category:"storage",term:"RAM",definition:"Temporary working memory for active datasets",example:"SQL query on 4GB of customer data requires at least 4GB RAM",aws:"EC2 Memory Optimized",azure:"Azure VM D-series",other:"-"},{category:"storage",term:"Disk Storage",definition:"Persistent storage medium for data",example:"Transaction logs stored on SSD for long-term retention",aws:"EBS, S3",azure:"Managed Disks, Blob Storage",other:"Google Persistent Disk"},{category:"storage",term:"SIMD Processing",definition:"CPU technique to process multiple data values in a single operation",example:"Comparing 256 transaction amounts to a threshold at once",aws:"-",azure:"-",other:"Intel AVX, ARM Neon"},{category:"storage",term:"Vectorized Execution",definition:"Processing batches of data at once rather than row-by-row",example:"Calculating average of 1000 transactions in a single operation",aws:"Redshift",azure:"Synapse Analytics",other:"Databricks Photon, Snowflake"},{category:"file",term:"CSV",definition:"Comma-separated values, simple text format for tabular data",example:"name,age,email\\nJohn,30,john@example.com",aws:"-",azure:"-",other:"-"},{category:"file",term:"JSON",definition:"JavaScript Object Notation, lightweight data-interchange format",example:'{"customer": {"name": "John", "orders": [{"id": 1}]}}',aws:"-",azure:"-",other:"-"},{category:"file",term:"Parquet",definition:"Columnar storage file format that compresses data",example:"A 100GB dataset reduced to 20GB with faster query times",aws:"AWS Glue, Athena",azure:"Synapse Analytics",other:"Databricks, Snowflake"},{category:"file",term:"Avro",definition:"Row-based storage format with schema definition",example:"Payment transactions with enforced data types",aws:"MSK (Kafka), Glue",azure:"HDInsight, Event Hubs",other:"Confluent Kafka, Databricks"},{category:"file",term:"ORC",definition:"Optimized Row Columnar format, optimized for Hive",example:"Data warehouse tables storing billions of transactions",aws:"EMR, Athena",azure:"HDInsight, Synapse",other:"Databricks, Hive"},{category:"file",term:"Delta Format",definition:"Open-source format that adds ACID transactions to Parquet",example:"Multiple data scientists updating the same dataset without conflicts",aws:"-",azure:"-",other:"Databricks Delta Lake"},{category:"database",term:"RDBMS",definition:"Relational Database Management System",example:"PostgreSQL storing customer information in tables",aws:"RDS, Aurora",azure:"Azure SQL DB",other:"Oracle, MySQL, PostgreSQL"},{category:"database",term:"NoSQL",definition:"Database design for non-tabular data models",example:"Storing customer profiles as flexible JSON documents",aws:"DynamoDB, DocumentDB",azure:"Cosmos DB",other:"MongoDB, Cassandra"},{category:"database",term:"OLTP",definition:"Online Transaction Processing systems",example:"Payment processing system handling 1000+ TPS",aws:"RDS, Aurora",azure:"Azure SQL DB",other:"Oracle, MySQL"},{category:"database",term:"OLAP",definition:"Online Analytical Processing systems",example:"Data warehouse generating quarterly reports",aws:"Redshift",azure:"Synapse Analytics",other:"Snowflake, Teradata"},{category:"database",term:"Column Storage",definition:"Storing data by organizing each column separately",example:"Storing all transaction amounts together for fast aggregation",aws:"Redshift",azure:"Synapse Analytics",other:"Snowflake, ClickHouse"},{category:"database",term:"Row Storage",definition:"Traditional storage method organizing data by rows",example:"Customer records stored as complete rows",aws:"RDS, Aurora",azure:"Azure SQL DB",other:"MySQL, PostgreSQL"},{category:"database",term:"Primary Key (PK)",definition:"Unique identifier column",example:"transaction_id in the transaction table",aws:"-",azure:"-",other:"-"},{category:"database",term:"Foreign Key (FK)",definition:"Column that references a primary key in another table",example:"merchant_id in transactions table",aws:"-",azure:"-",other:"-"},{category:"database",term:"Shard",definition:"A horizontal partition of data",example:"Customer data split across servers by region",aws:"DynamoDB, DocumentDB",azure:"Cosmos DB, SQL DB",other:"MongoDB Atlas, Citus"},{category:"database",term:"Partition",definition:"Division of a logical database into distinct parts",example:"Transaction table partitioned by month",aws:"Redshift, S3",azure:"Synapse, ADLS",other:"Snowflake, Hive"},{category:"modeling",term:"Schema",definition:"Structure that defines how data is organized",example:"Payment system tables for transactions, customers, merchants",aws:"Glue Data Catalog",azure:"Azure Purview",other:"Alation, Collibra"},{category:"modeling",term:"Star Schema",definition:"Dimensional model with a central fact table",example:"Transaction facts connected to customer, date dimensions",aws:"Redshift",azure:"Synapse Analytics",other:"Snowflake, BigQuery"},{category:"modeling",term:"Snowflake Schema",definition:"Extension of star schema with normalized dimensions",example:"Customer dimension normalized into details and address tables",aws:"Redshift",azure:"Synapse Analytics",other:"Snowflake, Oracle"},{category:"modeling",term:"Dimensional Modeling",definition:"Organizing data into measures and dimensions",example:"Payment amounts (measures) by time, location (dimensions)",aws:"-",azure:"-",other:"-"},{category:"modeling",term:"Data Vault",definition:"Enterprise data warehouse modeling methodology",example:"Hubs, links, and satellites for payment data",aws:"-",azure:"-",other:"-"},{category:"warehouse",term:"Data Warehouse",definition:"Repository for structured, filtered data",example:"Cleaned transaction data optimized for reporting",aws:"Redshift",azure:"Synapse Analytics",other:"Snowflake, BigQuery"},{category:"warehouse",term:"Data Lake",definition:"Repository for vast amounts of raw data",example:"S3 bucket with clickstream, logs, and customer interactions",aws:"S3 + Lake Formation",azure:"ADLS Gen2",other:"GCS + Dataproc"},{category:"warehouse",term:"Data Lakehouse",definition:"Hybrid approach combining lake storage with warehouse functionality",example:"Analyzing both structured and raw logs",aws:"Redshift Spectrum",azure:"Synapse Analytics",other:"Databricks Lakehouse"},{category:"warehouse",term:"ETL",definition:"Extract, Transform, Load process",example:"Extracting payment data, standardizing formats",aws:"Glue, Data Pipeline",azure:"Data Factory",other:"Informatica, Talend"},{category:"warehouse",term:"ELT",definition:"Extract, Load, Transform variation",example:"Loading raw logs to lake, transforming with SQL",aws:"Glue, Athena",azure:"Data Factory, Synapse",other:"Databricks, dbt"},{category:"warehouse",term:"Data Mart",definition:"Subset of a data warehouse for a specific business line",example:"Finance-specific view focused on revenue and chargebacks",aws:"Redshift",azure:"Synapse Analytics",other:"Snowflake"},{category:"processing",term:"Batch Processing",definition:"Processing data in large groups at intervals",example:"Daily job processing previous day's transactions",aws:"Batch, EMR",azure:"Batch Service, HDInsight",other:"Databricks, Spark"},{category:"processing",term:"Stream Processing",definition:"Processing data continuously as it's generated",example:"Real-time fraud detection for each transaction",aws:"Kinesis, MSK",azure:"Event Hubs, Stream Analytics",other:"Kafka, Flink"},{category:"processing",term:"Micro-batch",definition:"Processing data in small, near-continuous batches",example:"Aggregating transactions in 5-minute windows",aws:"Kinesis",azure:"Stream Analytics",other:"Spark Structured Streaming"},{category:"processing",term:"Real-time Processing",definition:"Processing data immediately as it arrives",example:"Instant payment notification to customer",aws:"Lambda + Kinesis",azure:"Functions + Event Hubs",other:"Flink, Kafka Streams"},{category:"processing",term:"MapReduce",definition:"Programming model for large distributed data processing",example:"Counting occurrences of each product in transactions",aws:"EMR",azure:"HDInsight",other:"Hadoop, Spark"},{category:"processing",term:"DAG (Directed Acyclic Graph)",definition:"Workflow with task dependencies",example:"ETL pipeline showing validation before transformation",aws:"Step Functions",azure:"Logic Apps",other:"Airflow, Prefect, Dagster"},{category:"cloud",term:"Distributed Computing",definition:"System with components on different networked computers",example:"Payment processing across multiple servers",aws:"-",azure:"-",other:"-"},{category:"cloud",term:"Horizontal Scaling",definition:"Adding more machines to handle increased load",example:"Adding 5 new database servers for Black Friday",aws:"Auto Scaling Groups",azure:"VMSS",other:"Kubernetes"},{category:"cloud",term:"Vertical Scaling",definition:"Adding more power to an existing machine",example:"Upgrading analytics server from 16GB to 64GB RAM",aws:"EC2 Instance Resize",azure:"VM Resize",other:"-"},{category:"cloud",term:"Container",definition:"Lightweight, standalone executable package",example:"Payment microservice in Docker container",aws:"ECS, EKS",azure:"AKS, Container Instances",other:"Docker, Kubernetes"},{category:"cloud",term:"Kubernetes",definition:"System for managing containerized applications",example:"Auto-scaling payment containers based on volume",aws:"EKS",azure:"AKS",other:"GKE, OpenShift"},{category:"cloud",term:"Microservices",definition:"Application as collection of loosely coupled services",example:"Separate services for payment processing, fraud detection",aws:"ECS, EKS, Lambda",azure:"AKS, Functions",other:"Kubernetes, Docker"},{category:"databricks",term:"Delta Lake",definition:"Storage layer with ACID transactions for big data",example:"Two analysts updating the same dataset without conflicts",aws:"-",azure:"-",other:"Databricks Delta Lake"},{category:"databricks",term:"Medallion Architecture",definition:"Bronze (raw), Silver (validated), Gold (aggregated) layers",example:"Raw JSON payments in Bronze, cleansed in Silver, summaries in Gold",aws:"-",azure:"-",other:"Databricks"},{category:"databricks",term:"Lakehouse",definition:"Hybrid architecture combining lake storage with warehouse functionality",example:"Petabytes of raw data with SQL query capabilities",aws:"-",azure:"-",other:"Databricks Lakehouse"},{category:"databricks",term:"Unity Catalog",definition:"Unified governance layer with fine-grained access controls",example:"Restricting PII access while allowing aggregate metrics",aws:"Glue/Lake Formation",azure:"Purview",other:"Databricks Unity Catalog"},{category:"databricks",term:"Photon Engine",definition:"C++ vectorized query engine for Spark",example:"Complex query running 5x faster with Photon",aws:"-",azure:"-",other:"Databricks Photon"},{category:"databricks",term:"Delta Live Tables (DLT)",definition:"Framework for reliable pipelines with quality controls",example:'Enforcing "all transaction_ids must be unique" rule',aws:"-",azure:"-",other:"Databricks DLT"},{category:"databricks",term:"MLflow",definition:"Platform for managing the ML lifecycle",example:"Tracking fraud detection model development parameters",aws:"SageMaker",azure:"Azure ML",other:"MLflow, Weights & Biases"},{category:"databricks",term:"Structured Streaming",definition:"Spark API for continuous data processing",example:"Processing payment events in real-time, updating models",aws:"-",azure:"-",other:"Spark Structured Streaming"},{category:"databricks",term:"Z-Ordering",definition:"Technique that co-locates related data for faster queries",example:"Z-ordering by date and merchant_id for 10-100x speed",aws:"-",azure:"-",other:"Databricks"},{category:"databricks",term:"DBUs (Databricks Units)",definition:"Billing units for compute resources",example:"4-core cluster consuming 0.75 DBUs per hour",aws:"-",azure:"-",other:"Databricks"},{category:"databricks",term:"Notebooks",definition:"Interactive documents with code, visualizations, markdown",example:"End-to-end payment analysis notebook",aws:"SageMaker Notebooks",azure:"Azure Notebooks",other:"Jupyter, Databricks, Colab"},{category:"databricks",term:"Workspace",definition:"Collaborative environment for organizing assets",example:"Organizing notebooks into department folders",aws:"-",azure:"-",other:"Databricks Workspace"},{category:"databricks",term:"Clusters",definition:"Managed compute resources for workloads",example:"Auto-scaling cluster for peak processing",aws:"EMR",azure:"HDInsight, Synapse",other:"Databricks"},{category:"databricks",term:"Auto Loader",definition:"Ingestion service for incrementally processing new files",example:"Automatically processing new transaction log files",aws:"-",azure:"-",other:"Databricks Auto Loader"},{category:"advanced",term:"Data Lineage",definition:"Documentation of data's origins and transformations",example:"Tracing payment metric back to source systems",aws:"Glue, AWS CloudTrail",azure:"Purview",other:"Collibra, Alation"},{category:"advanced",term:"Data Governance",definition:"Framework for managing data availability, security",example:"Policies for PII access and protection",aws:"Lake Formation",azure:"Purview",other:"Collibra, Alation"},{category:"advanced",term:"Data Catalog",definition:"Inventory of available data assets",example:"Directory of payment datasets with descriptions",aws:"Glue Data Catalog",azure:"Purview",other:"Collibra, Alation"},{category:"advanced",term:"Data Quality",definition:"Measure of data's condition and suitability",example:"Percentage of transactions with valid customer_ids",aws:"Glue DataBrew",azure:"Data Factory Data Flows",other:"Great Expectations, dbt"},{category:"advanced",term:"Schema Evolution",definition:"Changing schema without disrupting services",example:"Adding payment_method field without breaking pipelines",aws:"Glue Schema Registry",azure:"Schema Registry",other:"Confluent Schema Registry"},{category:"advanced",term:"CDC (Change Data Capture)",definition:"Capturing changes made to a database",example:"Tracking all customer address updates",aws:"DMS",azure:"Data Factory",other:"Debezium, Striim"},{category:"advanced",term:"Idempotence",definition:"Operation can be applied multiple times without changing result",example:"Payment processing that doesn't double-charge",aws:"-",azure:"-",other:"-"},{category:"advanced",term:"ACID Properties",definition:"Guaranteeing database transactions",example:"Funds transfer completes entirely or not at all",aws:"-",azure:"-",other:"-"},{category:"advanced",term:"Slowly Changing Dimensions",definition:"Handling historical data changes in a warehouse",example:"Tracking merchant information changes over time",aws:"-",azure:"-",other:"-"},{category:"platform",term:"S3 (AWS)",definition:"Simple Storage Service, object storage",example:"Storing transaction logs in encrypted bucket",aws:"S3",azure:"-",other:"-"},{category:"platform",term:"Blob Storage (Azure)",definition:"Microsoft's object storage solution",example:"Archiving payment receipts",aws:"-",azure:"Blob Storage",other:"-"},{category:"platform",term:"Redshift (AWS)",definition:"Cloud data warehouse service",example:"Weekly settlement reports across merchant data",aws:"Redshift",azure:"-",other:"-"},{category:"platform",term:"Synapse Analytics (Azure)",definition:"Analytics service for data warehousing and big data",example:"Combining transactions with clickstream analysis",aws:"-",azure:"Synapse Analytics",other:"-"},{category:"platform",term:"Glue (AWS)",definition:"Serverless data integration service",example:"Cataloging payment data sources",aws:"Glue",azure:"-",other:"-"},{category:"platform",term:"Data Factory (Azure)",definition:"Integration service for ETL/ELT workflows",example:"Moving data from payment systems to data lake",aws:"-",azure:"Data Factory",other:"-"},{category:"platform",term:"Athena (AWS)",definition:"Interactive query service for S3 data",example:"Ad-hoc fraud analysis queries against S3 data",aws:"Athena",azure:"-",other:"-"},{category:"platform",term:"EMR (AWS)",definition:"Managed Hadoop framework",example:"Monthly reconciliation across historical data",aws:"EMR",azure:"-",other:"-"},{category:"platform",term:"Lambda (AWS)",definition:"Serverless compute service",example:"Real-time payment validation functions",aws:"Lambda",azure:"-",other:"-"},{category:"platform",term:"Azure Functions",definition:"Microsoft's serverless compute",example:"Fraud detection triggers on transactions",aws:"-",azure:"Azure Functions",other:"-"}].filter(a=>{const i="all"===e||a.category===e,r=a.term.toLowerCase().includes(t.toLowerCase())||a.definition.toLowerCase().includes(t.toLowerCase())||a.example.toLowerCase().includes(t.toLowerCase())||a.aws.toLowerCase().includes(t.toLowerCase())||a.azure.toLowerCase().includes(t.toLowerCase())||a.other.toLowerCase().includes(t.toLowerCase());return i&&r});return r.a.createElement("div",{className:"flex flex-col max-w-6xl mx-auto p-4"},r.a.createElement("div",{className:"bg-gradient-to-r from-blue-600 to-purple-600 p-6 rounded-t-lg text-center"},r.a.createElement("h1",{className:"text-2xl font-bold text-white"},"Data Engineering Glossary"),r.a.createElement("p",{className:"text-white opacity-80 mt-2"},"From Bits to Databricks: Comprehensive Data Terms with Platform Examples")),r.a.createElement("div",{className:"bg-white p-4 border-b flex flex-col md:flex-row gap-4"},r.a.createElement("div",{className:"flex-1"},r.a.createElement("input",{type:"text",placeholder:"Search terms, definitions, examples...",className:"w-full p-2 border rounded",value:t,onChange:e=>o(e.target.value)})),r.a.createElement("div",{className:"flex flex-wrap gap-2"},[{id:"all",name:"All Categories"},{id:"fundamental",name:"Fundamental Concepts"},{id:"storage",name:"Storage & Computing"},{id:"file",name:"File Formats"},{id:"database",name:"Database Concepts"},{id:"modeling",name:"Data Modeling"},{id:"warehouse",name:"Warehouses & Lakes"},{id:"processing",name:"Data Processing"},{id:"cloud",name:"Cloud Computing"},{id:"databricks",name:"Databricks"},{id:"advanced",name:"Advanced Concepts"},{id:"platform",name:"Platform-Specific"}].map(t=>r.a.createElement("button",{key:t.id,onClick:()=>a(t.id),className:`px-3 py-1 rounded text-sm ${e===t.id?"bg-blue-600 text-white":"bg-gray-200 text-gray-800"}`},t.name)))),r.a.createElement("div",{className:"bg-white shadow-lg rounded-b-lg overflow-hidden"},r.a.createElement("table",{className:"min-w-full divide-y divide-gray-200"},r.a.createElement("thead",{className:"bg-gray-50"},r.a.createElement("tr",null,r.a.createElement("th",{scope:"col",className:"p-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider"},"Term"),r.a.createElement("th",{scope:"col",className:"p-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider"},"Definition"),r.a.createElement("th",{scope:"col",className:"p-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider"},"Example"),r.a.createElement("th",{scope:"col",className:"p-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider"},"AWS"),r.a.createElement("th",{scope:"col",className:"p-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider"},"Azure"),r.a.createElement("th",{scope:"col",className:"p-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider"},"Other"))),r.a.createElement("tbody",{className:"bg-white divide-y divide-gray-200"},n.length>0?n.map((e,a)=>r.a.createElement(r.a.Fragment,{key:e.term},r.a.createElement("tr",{className:`${a%2===0?"bg-white":"bg-gray-50"}`},r.a.createElement("td",{className:"p-3 whitespace-nowrap font-medium text-gray-900"},e.term),r.a.createElement("td",{className:"p-3 text-gray-700"},e.definition),r.a.createElement("td",{className:"p-3 text-gray-700"},e.example),r.a.createElement("td",{className:"p-3 text-gray-700 whitespace-nowrap"},e.aws),r.a.createElement("td",{className:"p-3 text-gray-700 whitespace-nowrap"},e.azure),r.a.createElement("td",{className:"p-3 text-gray-700 whitespace-nowrap"},e.other)))):r.a.createElement("tr",null,r.a.createElement("td",{colSpan:"6",className:"p-4 text-center text-gray-500"},"No terms found matching your criteria. Try adjusting your search or filter."))))),r.a.createElement("div",{className:"mt-6 bg-white p-4 rounded shadow"},r.a.createElement("h2",{className:"text-lg font-semibold text-gray-800 mb-2"},"Using This Glossary"),r.a.createElement("p",{className:"text-gray-700 mb-2"},"This glossary provides comprehensive coverage of data engineering terms from fundamental concepts to platform-specific implementations."),r.a.createElement("p",{className:"text-gray-700"},"Click on any row to see expanded details. Use the category filters and search bar to quickly find specific terms.")))};t(14);var l=function(){const[e,a]=Object(i.useState)("comparison");return r.a.createElement("div",{className:"App"},r.a.createElement("div",{className:"bg-gradient-to-r from-blue-600 to-purple-600 p-4 flex justify-center space-x-4"},r.a.createElement("button",{onClick:()=>a("comparison"),className:`px-4 py-2 rounded ${"comparison"===e?"bg-white text-blue-600":"bg-transparent text-white"}`},"Database Comparison"),r.a.createElement("button",{onClick:()=>a("glossary"),className:`px-4 py-2 rounded ${"glossary"===e?"bg-white text-purple-600":"bg-transparent text-white"}`},"Data Engineering Glossary")),"comparison"===e&&r.a.createElement(s,null),"glossary"===e&&r.a.createElement(c,null))};var d=e=>{e&&e instanceof Function&&t.e(3).then(t.bind(null,16)).then(a=>{let{getCLS:t,getFID:i,getFCP:r,getLCP:o,getTTFB:n}=a;t(e),i(e),r(e),o(e),n(e)})};n.a.createRoot(document.getElementById("root")).render(r.a.createElement(r.a.StrictMode,null,r.a.createElement(l,null))),d()},4:function(e,a,t){e.exports=t(15)}},[[4,1,2]]]);
//# sourceMappingURL=main.532bdb72.chunk.js.map